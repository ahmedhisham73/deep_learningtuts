{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Romeo_and_julliet.ipy",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedhisham73/deep_learningtuts/blob/master/Romeo_and_julliet_ipy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFnbfw8Rmne",
        "colab_type": "code",
        "outputId": "f674d00f-756e-4489-f0bd-f5d2ad8c9d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/shakes2.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  162329\n",
            "Total Vocab:  64\n",
            "Total Patterns:  162229\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/20\n",
            "162229/162229 [==============================] - 167s 1ms/step - loss: 3.0013\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.00126, saving model to weights-improvement-01-3.0013.hdf5\n",
            "Epoch 2/20\n",
            "162229/162229 [==============================] - 161s 991us/step - loss: 2.8351\n",
            "\n",
            "Epoch 00002: loss improved from 3.00126 to 2.83508, saving model to weights-improvement-02-2.8351.hdf5\n",
            "Epoch 3/20\n",
            "162229/162229 [==============================] - 161s 993us/step - loss: 2.7557\n",
            "\n",
            "Epoch 00003: loss improved from 2.83508 to 2.75571, saving model to weights-improvement-03-2.7557.hdf5\n",
            "Epoch 4/20\n",
            "162229/162229 [==============================] - 159s 983us/step - loss: 2.6889\n",
            "\n",
            "Epoch 00004: loss improved from 2.75571 to 2.68886, saving model to weights-improvement-04-2.6889.hdf5\n",
            "Epoch 5/20\n",
            "162229/162229 [==============================] - 160s 988us/step - loss: 2.6320\n",
            "\n",
            "Epoch 00005: loss improved from 2.68886 to 2.63195, saving model to weights-improvement-05-2.6320.hdf5\n",
            "Epoch 6/20\n",
            "162229/162229 [==============================] - 159s 981us/step - loss: 2.5831\n",
            "\n",
            "Epoch 00006: loss improved from 2.63195 to 2.58311, saving model to weights-improvement-06-2.5831.hdf5\n",
            "Epoch 7/20\n",
            "162229/162229 [==============================] - 161s 994us/step - loss: 2.5403\n",
            "\n",
            "Epoch 00007: loss improved from 2.58311 to 2.54033, saving model to weights-improvement-07-2.5403.hdf5\n",
            "Epoch 8/20\n",
            "162229/162229 [==============================] - 158s 977us/step - loss: 2.5031\n",
            "\n",
            "Epoch 00008: loss improved from 2.54033 to 2.50310, saving model to weights-improvement-08-2.5031.hdf5\n",
            "Epoch 9/20\n",
            "162229/162229 [==============================] - 158s 972us/step - loss: 2.4705\n",
            "\n",
            "Epoch 00009: loss improved from 2.50310 to 2.47051, saving model to weights-improvement-09-2.4705.hdf5\n",
            "Epoch 10/20\n",
            "162229/162229 [==============================] - 157s 967us/step - loss: 2.4394\n",
            "\n",
            "Epoch 00010: loss improved from 2.47051 to 2.43945, saving model to weights-improvement-10-2.4394.hdf5\n",
            "Epoch 11/20\n",
            "162229/162229 [==============================] - 157s 967us/step - loss: 2.4120\n",
            "\n",
            "Epoch 00011: loss improved from 2.43945 to 2.41201, saving model to weights-improvement-11-2.4120.hdf5\n",
            "Epoch 12/20\n",
            "162229/162229 [==============================] - 157s 967us/step - loss: 2.3842\n",
            "\n",
            "Epoch 00012: loss improved from 2.41201 to 2.38417, saving model to weights-improvement-12-2.3842.hdf5\n",
            "Epoch 13/20\n",
            "162229/162229 [==============================] - 157s 968us/step - loss: 2.3579\n",
            "\n",
            "Epoch 00013: loss improved from 2.38417 to 2.35785, saving model to weights-improvement-13-2.3579.hdf5\n",
            "Epoch 14/20\n",
            "162229/162229 [==============================] - 157s 965us/step - loss: 2.3364\n",
            "\n",
            "Epoch 00014: loss improved from 2.35785 to 2.33639, saving model to weights-improvement-14-2.3364.hdf5\n",
            "Epoch 15/20\n",
            "162229/162229 [==============================] - 157s 967us/step - loss: 2.3123\n",
            "\n",
            "Epoch 00015: loss improved from 2.33639 to 2.31231, saving model to weights-improvement-15-2.3123.hdf5\n",
            "Epoch 16/20\n",
            "162229/162229 [==============================] - 156s 963us/step - loss: 2.2908\n",
            "\n",
            "Epoch 00016: loss improved from 2.31231 to 2.29084, saving model to weights-improvement-16-2.2908.hdf5\n",
            "Epoch 17/20\n",
            "162229/162229 [==============================] - 157s 965us/step - loss: 2.2711\n",
            "\n",
            "Epoch 00017: loss improved from 2.29084 to 2.27112, saving model to weights-improvement-17-2.2711.hdf5\n",
            "Epoch 18/20\n",
            "162229/162229 [==============================] - 158s 976us/step - loss: 2.2519\n",
            "\n",
            "Epoch 00018: loss improved from 2.27112 to 2.25189, saving model to weights-improvement-18-2.2519.hdf5\n",
            "Epoch 19/20\n",
            "162229/162229 [==============================] - 159s 981us/step - loss: 2.2335\n",
            "\n",
            "Epoch 00019: loss improved from 2.25189 to 2.23349, saving model to weights-improvement-19-2.2335.hdf5\n",
            "Epoch 20/20\n",
            "162229/162229 [==============================] - 158s 974us/step - loss: 2.2172\n",
            "\n",
            "Epoch 00020: loss improved from 2.23349 to 2.21724, saving model to weights-improvement-20-2.2172.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88ee138fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nli5axESZSfr",
        "colab_type": "code",
        "outputId": "7c63bd04-bce1-4930-dba8-841009d7e6c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/shakes2.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"/content/weights-improvement-20-2.2172.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  162329\n",
            "Total Vocab:  64\n",
            "Total Patterns:  162229\n",
            "Seed:\n",
            "\" e that cannot lick his fingers goes not with me.\n",
            "\n",
            "capulet.\n",
            "go, begone.\n",
            "\n",
            " [_exit second servant._]\n",
            "\n",
            "w \"\n",
            "hat is the world toat the hart to the would to the worl"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}