{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alice_in_wonderland.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedhisham73/deep_learningtuts/blob/master/alice_in_wonderland.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBQBW3bdDmeO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0aefa2e6-2583-4f5c-9c31-78ba0c3bf1d5"
      },
      "source": [
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "8\n",
        "9\n",
        "10\n",
        "11\n",
        "12\n",
        "13\n",
        "14\n",
        "15\n",
        "16\n",
        "17\n",
        "18\n",
        "19\n",
        "20\n",
        "21\n",
        "22\n",
        "23\n",
        "24\n",
        "25\n",
        "26\n",
        "27\n",
        "28\n",
        "29\n",
        "30\n",
        "31\n",
        "32\n",
        "33\n",
        "34\n",
        "35\n",
        "36\n",
        "37\n",
        "38\n",
        "39\n",
        "40\n",
        "41\n",
        "42\n",
        "43\n",
        "44\n",
        "45\n",
        "46\n",
        "47\n",
        "48\n",
        "49\n",
        "\t\n",
        "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  163781\n",
            "Total Vocab:  59\n",
            "Total Patterns:  163681\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/20\n",
            "163681/163681 [==============================] - 163s 993us/step - loss: 2.9759\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.97587, saving model to weights-improvement-01-2.9759.hdf5\n",
            "Epoch 2/20\n",
            "163681/163681 [==============================] - 158s 966us/step - loss: 2.7971\n",
            "\n",
            "Epoch 00002: loss improved from 2.97587 to 2.79705, saving model to weights-improvement-02-2.7971.hdf5\n",
            "Epoch 3/20\n",
            "163681/163681 [==============================] - 158s 967us/step - loss: 2.7134\n",
            "\n",
            "Epoch 00003: loss improved from 2.79705 to 2.71336, saving model to weights-improvement-03-2.7134.hdf5\n",
            "Epoch 4/20\n",
            "163681/163681 [==============================] - 159s 973us/step - loss: 2.6443\n",
            "\n",
            "Epoch 00004: loss improved from 2.71336 to 2.64426, saving model to weights-improvement-04-2.6443.hdf5\n",
            "Epoch 5/20\n",
            "163681/163681 [==============================] - 160s 979us/step - loss: 2.5852\n",
            "\n",
            "Epoch 00005: loss improved from 2.64426 to 2.58525, saving model to weights-improvement-05-2.5852.hdf5\n",
            "Epoch 6/20\n",
            "163681/163681 [==============================] - 163s 997us/step - loss: 2.5277\n",
            "\n",
            "Epoch 00006: loss improved from 2.58525 to 2.52766, saving model to weights-improvement-06-2.5277.hdf5\n",
            "Epoch 7/20\n",
            "163681/163681 [==============================] - 163s 994us/step - loss: 2.4735\n",
            "\n",
            "Epoch 00007: loss improved from 2.52766 to 2.47353, saving model to weights-improvement-07-2.4735.hdf5\n",
            "Epoch 8/20\n",
            "163681/163681 [==============================] - 161s 982us/step - loss: 2.4248\n",
            "\n",
            "Epoch 00008: loss improved from 2.47353 to 2.42480, saving model to weights-improvement-08-2.4248.hdf5\n",
            "Epoch 9/20\n",
            "163681/163681 [==============================] - 158s 968us/step - loss: 2.3802\n",
            "\n",
            "Epoch 00009: loss improved from 2.42480 to 2.38016, saving model to weights-improvement-09-2.3802.hdf5\n",
            "Epoch 10/20\n",
            "163681/163681 [==============================] - 159s 969us/step - loss: 2.3368\n",
            "\n",
            "Epoch 00010: loss improved from 2.38016 to 2.33680, saving model to weights-improvement-10-2.3368.hdf5\n",
            "Epoch 11/20\n",
            "163681/163681 [==============================] - 159s 969us/step - loss: 2.2996\n",
            "\n",
            "Epoch 00011: loss improved from 2.33680 to 2.29960, saving model to weights-improvement-11-2.2996.hdf5\n",
            "Epoch 12/20\n",
            "163681/163681 [==============================] - 157s 961us/step - loss: 2.2604\n",
            "\n",
            "Epoch 00012: loss improved from 2.29960 to 2.26038, saving model to weights-improvement-12-2.2604.hdf5\n",
            "Epoch 13/20\n",
            "163681/163681 [==============================] - 157s 962us/step - loss: 2.2267\n",
            "\n",
            "Epoch 00013: loss improved from 2.26038 to 2.22666, saving model to weights-improvement-13-2.2267.hdf5\n",
            "Epoch 14/20\n",
            "163681/163681 [==============================] - 158s 966us/step - loss: 2.1936\n",
            "\n",
            "Epoch 00014: loss improved from 2.22666 to 2.19360, saving model to weights-improvement-14-2.1936.hdf5\n",
            "Epoch 15/20\n",
            "163681/163681 [==============================] - 157s 960us/step - loss: 2.1618\n",
            "\n",
            "Epoch 00015: loss improved from 2.19360 to 2.16181, saving model to weights-improvement-15-2.1618.hdf5\n",
            "Epoch 16/20\n",
            "163681/163681 [==============================] - 158s 966us/step - loss: 2.1331\n",
            "\n",
            "Epoch 00016: loss improved from 2.16181 to 2.13314, saving model to weights-improvement-16-2.1331.hdf5\n",
            "Epoch 17/20\n",
            "163681/163681 [==============================] - 158s 965us/step - loss: 2.1039\n",
            "\n",
            "Epoch 00017: loss improved from 2.13314 to 2.10394, saving model to weights-improvement-17-2.1039.hdf5\n",
            "Epoch 18/20\n",
            "163681/163681 [==============================] - 159s 969us/step - loss: 2.0773\n",
            "\n",
            "Epoch 00018: loss improved from 2.10394 to 2.07734, saving model to weights-improvement-18-2.0773.hdf5\n",
            "Epoch 19/20\n",
            "163681/163681 [==============================] - 157s 960us/step - loss: 2.0514\n",
            "\n",
            "Epoch 00019: loss improved from 2.07734 to 2.05136, saving model to weights-improvement-19-2.0514.hdf5\n",
            "Epoch 20/20\n",
            "163681/163681 [==============================] - 157s 962us/step - loss: 2.0282\n",
            "\n",
            "Epoch 00020: loss improved from 2.05136 to 2.02822, saving model to weights-improvement-20-2.0282.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1c6a8ff28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abAEipapDs5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e9a7e1f3-1540-4fb3-ead6-b943c81c08a0"
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"/content/weights-improvement-20-2.0282.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  163781\n",
            "Total Vocab:  59\n",
            "Total Patterns:  163681\n",
            "Seed:\n",
            "\"  whiskers, how late it's getting!' she was close behind it when she\n",
            "turned the corner, but the rabbi \"\n",
            "t sas soe ths th the tabte tas if toe was so tee that was no the tooe, and the woide harde aale toin it and toeereng and the crure and the care was iet herd to the tooe, and the wert oo the kaste har an tee rooee and the care wfsh all hrre oo the sabdit wo te the sabee and the woide aadit the was and toene and the caree tf the care and the caree if the karter wat if d lothe aflit in a lotge of the sohen. and the was aolin the tas and the car. \n",
            "'io wost het to tee soieg ' said the mock turtle. ''ie course toe taid the gorsouse ' taid the mock turtle. ''ie tou the moth of the toile of the sooet? ' shiught alice, 'i monw the seme the harter wfat iar tee woide and the sabden to tee ftor on the harte \n",
            "and tee would hate to the toiee an her foo the rooee aadut in a lottee and the care and the care and the care wfsh a little balut th the tase whi had soted aedin the was afdin, and the west on aelin an ince oote the rooee aadkt th the toeee of the haree hare tee sabdit soted an tee sooee. and \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}